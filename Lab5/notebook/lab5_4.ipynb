{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NinEk1AgZh-K",
        "outputId": "f9308b04-56f8-454b-ffcd-85bfc4bc133f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 23625\n",
            "NER tag size: 9\n",
            "Example sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "NER ids: [3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
            "NER tags: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "#Task 1\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "dataset = load_dataset(\"lhoestq/conll2003\")\n",
        "\n",
        "# Extract sentences and ner tags\n",
        "train_tokens = dataset[\"train\"][\"tokens\"]\n",
        "train_ner_tags = dataset[\"train\"][\"ner_tags\"]\n",
        "val_tokens = dataset[\"validation\"][\"tokens\"]\n",
        "val_ner_tags = dataset[\"validation\"][\"ner_tags\"]\n",
        "\n",
        "# Manual mapping for CoNLL-2003\n",
        "ner_label_names = [\n",
        "    \"O\",\n",
        "    \"B-PER\", \"I-PER\",\n",
        "    \"B-ORG\", \"I-ORG\",\n",
        "    \"B-LOC\", \"I-LOC\",\n",
        "    \"B-MISC\", \"I-MISC\"\n",
        "]\n",
        "\n",
        "# Build word vocab\n",
        "word_counter = Counter()\n",
        "for sent in train_tokens:\n",
        "    word_counter.update(sent)\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "UNK = \"<UNK>\"\n",
        "vocab = [PAD, UNK] + sorted(word_counter.keys())\n",
        "word_to_ix = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "# Build tag vocab\n",
        "tag_to_ix = {t: i for i, t in enumerate(ner_label_names)}\n",
        "\n",
        "train_tags_str = [\n",
        "    [ner_label_names[idx] for idx in seq]\n",
        "    for seq in train_ner_tags\n",
        "]\n",
        "\n",
        "print(\"Vocab size:\", len(word_to_ix))\n",
        "print(\"NER tag size:\", len(tag_to_ix))\n",
        "print(\"Example sentence:\", train_tokens[0])\n",
        "print(\"NER ids:\", train_ner_tags[0])\n",
        "print(\"NER tags:\", train_tags_str[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLFNMf6faKV2",
        "outputId": "cda18a77-bcfa-4f69-bc26-d19690c214fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 38])\n",
            "torch.Size([32, 38])\n"
          ]
        }
      ],
      "source": [
        "#Task 2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.UNK = word_to_ix[\"<UNK>\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.sentences[idx]\n",
        "        labels = self.tags[idx]\n",
        "        word_ids = torch.tensor([self.word_to_ix.get(w, self.UNK) for w in words])\n",
        "        label_ids = torch.tensor([label for label in labels])\n",
        "        return word_ids, label_ids\n",
        "PAD_IDX = word_to_ix[\"<PAD>\"]\n",
        "TAG_PAD = -1\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sents = [item[0] for item in batch]\n",
        "    tags = [item[1] for item in batch]\n",
        "    sents_padded = pad_sequence(sents, batch_first=True, padding_value=PAD_IDX)\n",
        "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=TAG_PAD)\n",
        "    return sents_padded, tags_padded\n",
        "train_dataset = NERDataset(train_tokens, train_ner_tags, word_to_ix, tag_to_ix)\n",
        "val_dataset = NERDataset(val_tokens, val_ner_tags, word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "batch_sents, batch_tags = next(iter(train_loader))\n",
        "print(batch_sents.shape)\n",
        "print(batch_tags.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3ob6j31ampv",
        "outputId": "90911a0f-1889-478e-8ecc-da923ef8f5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NERTagger(\n",
            "  (embedding): Embedding(23625, 100, padding_idx=0)\n",
            "  (lstm): LSTM(100, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Task 3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NERTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_size = len(tag_to_ix)\n",
        "pad_idx = word_to_ix[\"<PAD>\"]\n",
        "\n",
        "model = NERTagger(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_size=output_size,\n",
        "    pad_idx=pad_idx\n",
        ")\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCQZviZa3fT",
        "outputId": "86f5035e-8ced-401b-a830-0eae017ce0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Loss: 0.6817\n",
            "Epoch 2/5 | Loss: 0.3596\n",
            "Epoch 3/5 | Loss: 0.2310\n",
            "Epoch 4/5 | Loss: 0.1565\n",
            "Epoch 5/5 | Loss: 0.1064\n"
          ]
        }
      ],
      "source": [
        "#Task 4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_sents, batch_tags in train_loader:\n",
        "        batch_sents = batch_sents.to(device)\n",
        "        batch_tags = batch_tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_sents)\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            batch_tags.view(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYgXavBEbySv",
        "outputId": "b1835a73-f7a7-40da-fe0f-e5500e9876b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9364510727775398\n",
            "VNU B-ORG\n",
            "University B-ORG\n",
            "is O\n",
            "located O\n",
            "in O\n",
            "Hanoi O\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Task 5\n",
        "import torch\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_sents, batch_tags in dataloader:\n",
        "            batch_sents = batch_sents.to(device)\n",
        "            batch_tags = batch_tags.to(device)\n",
        "\n",
        "            logits = model(batch_sents)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            mask = batch_tags != TAG_PAD\n",
        "            correct += (preds[mask] == batch_tags[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "val_acc = evaluate(model, val_loader)\n",
        "print(\"Validation accuracy:\", val_acc)\n",
        "\n",
        "\n",
        "def predict_sentence(sentence):\n",
        "    tokens = sentence.split()\n",
        "    ids = [word_to_ix.get(w, word_to_ix[\"<UNK>\"]) for w in tokens]\n",
        "    x = torch.tensor(ids).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "\n",
        "    tags = [ner_label_names[i] for i in preds]\n",
        "    for w, t in zip(tokens, tags):\n",
        "        print(w, t)\n",
        "\n",
        "print(predict_sentence(\"VNU University is located in Hanoi\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-voQEWfcbVG"
      },
      "source": [
        "• Độ chính xác trên tập validation: 0.9364510727775398\n",
        "• Ví dụ dự đoán câu mới:\n",
        "\n",
        "Câu: “VNU University is located in Hanoi”\n",
        "Dự đoán: VNU B-ORG\n",
        "University B-ORG\n",
        "is O\n",
        "located O\n",
        "in O\n",
        "Hanoi O"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

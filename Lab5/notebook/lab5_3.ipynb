{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3vKlqFWBMPL",
        "outputId": "a9fb0dae-058c-4178-ca0b-d8b955fb1246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 20201\n",
            "Tag set size: 18\n",
            "Example pairs: [('Al', 'PROPN'), ('-', 'PUNCT'), ('Zaman', 'PROPN'), (':', 'PUNCT'), ('American', 'ADJ'), ('forces', 'NOUN'), ('killed', 'VERB'), ('Shaikh', 'PROPN'), ('Abdullah', 'PROPN'), ('al', 'PROPN')]\n"
          ]
        }
      ],
      "source": [
        "#Task 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def load_conllu(file_path):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "                continue\n",
        "            if line.startswith(\"#\"):\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) > 3:\n",
        "                word = parts[1]\n",
        "                tag = parts[3]\n",
        "                current_sentence.append((word, tag))\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence)\n",
        "    return sentences\n",
        "\n",
        "train_data = load_conllu(\"/content/en_ewt-ud-train.conllu\")\n",
        "dev_data = load_conllu(\"/content/en_ewt-ud-dev.conllu\")\n",
        "\n",
        "word_to_ix = {\"<UNK>\": 0}\n",
        "tag_to_ix = {}\n",
        "\n",
        "for sentence in train_data:\n",
        "    for word, tag in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "print(\"Vocabulary size:\", len(word_to_ix))\n",
        "print(\"Tag set size:\", len(tag_to_ix))\n",
        "print(\"Example pairs:\", train_data[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMZsBW3NCRIq",
        "outputId": "1e8a8980-1c95-4a37-e2cf-be38967415ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sentence indices: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
            "Example tag indices: tensor([0, 1, 0, 1, 2, 3, 4, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "#Task 2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        words = [self.word_to_ix.get(w, self.word_to_ix[\"<UNK>\"]) for w, _ in sentence]\n",
        "        tags = [self.tag_to_ix[t] for _, t in sentence]\n",
        "        return torch.tensor(words, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    sentences_padded = pad_sequence(sentences, batch_first=True)\n",
        "    tags_padded = pad_sequence(tags, batch_first=True)\n",
        "    return sentences_padded, tags_padded\n",
        "\n",
        "train_dataset = POSDataset(train_data, word_to_ix, tag_to_ix)\n",
        "dev_dataset = POSDataset(dev_data, word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "sample_sent, sample_tag = train_dataset[0]\n",
        "print(\"Example sentence indices:\", sample_sent[:10])\n",
        "print(\"Example tag indices:\", sample_tag[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg1ML3VmD47A",
        "outputId": "20c7beeb-922a-4b18-f4ff-7329e886ce6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch shape: torch.Size([32, 48])\n",
            "Logits shape: torch.Size([32, 48, 18])\n"
          ]
        }
      ],
      "source": [
        "#Task 3\n",
        "class SimpleRNNForTokenClassification(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_size, embedding_dim=128, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        logits = self.fc(output)\n",
        "        return logits\n",
        "\n",
        "model = SimpleRNNForTokenClassification(len(word_to_ix), len(tag_to_ix))\n",
        "x_example, y_example = next(iter(train_loader))\n",
        "logits_example = model(x_example)\n",
        "\n",
        "print(\"Input batch shape:\", x_example.shape)\n",
        "print(\"Logits shape:\", logits_example.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrgKJUhjD-sf",
        "outputId": "baaf7fda-08bd-4436-b0a9-053e61a0a2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.9703491297455467\n",
            "Epoch 2 Loss: 0.4999715414430414\n",
            "Epoch 3 Loss: 0.3698372770176858\n",
            "Sample input indices: tensor([1559,   13, 9360, 3111,   74,  757,    4,    0,    0,    0])\n",
            "Sample true tags: tensor([6, 5, 0, 4, 5, 3, 1, 0, 0, 0])\n",
            "Sample predicted tags: tensor([6, 5, 3, 3, 5, 3, 1, 4, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "#Task 4\n",
        "model = SimpleRNNForTokenClassification(len(word_to_ix), len(tag_to_ix))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(\"Epoch\", epoch + 1, \"Loss:\", total_loss / len(train_loader))\n",
        "\n",
        "x_sample, y_sample = next(iter(dev_loader))\n",
        "logits_sample = model(x_sample)\n",
        "pred_sample = torch.argmax(logits_sample, dim=-1)\n",
        "\n",
        "print(\"Sample input indices:\", x_sample[0][:10])\n",
        "print(\"Sample true tags:\", y_sample[0][:10])\n",
        "print(\"Sample predicted tags:\", pred_sample[0][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BS0vRl4EjGG",
        "outputId": "2f12c39c-fca4-4a53-e0eb-3da22cfdbdfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev accuracy: 0.8633087460484721\n",
            "Example true tags: tensor([6, 5, 0, 4, 5, 3, 1, 0, 0, 0])\n",
            "Example predicted tags: tensor([6, 5, 3, 3, 5, 3, 1, 4, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "#Task 5\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            logits = model(batch_x)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            mask = batch_y != 0\n",
        "            correct += (preds[mask] == batch_y[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "    return correct / total\n",
        "\n",
        "dev_acc = evaluate(model, dev_loader)\n",
        "print(\"Dev accuracy:\", dev_acc)\n",
        "\n",
        "x_sample, y_sample = next(iter(dev_loader))\n",
        "logits_sample = model(x_sample)\n",
        "pred_sample = torch.argmax(logits_sample, dim=-1)\n",
        "\n",
        "print(\"Example true tags:\", y_sample[0][:10])\n",
        "print(\"Example predicted tags:\", pred_sample[0][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd3XDpxTGPaX",
        "outputId": "4ec4aa83-4a5f-4266-b1af-c4f941dd7a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss=0.9911 | Train Acc=0.7048 | Dev Acc=0.8074\n",
            "Epoch 2: Loss=0.5086 | Train Acc=0.8412 | Dev Acc=0.8508\n",
            "Epoch 3: Loss=0.3749 | Train Acc=0.8818 | Dev Acc=0.8724\n"
          ]
        }
      ],
      "source": [
        "best_dev_acc = 0.0\n",
        "train_acc_list = []\n",
        "dev_acc_list = []\n",
        "\n",
        "model = SimpleRNNForTokenClassification(len(word_to_ix), len(tag_to_ix))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_x)\n",
        "\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        mask = batch_y != 0\n",
        "        total_correct += (preds[mask] == batch_y[mask]).sum().item()\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "    train_acc = total_correct / total_tokens\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    dev_acc = evaluate(model, dev_loader)\n",
        "    dev_acc_list.append(dev_acc)\n",
        "\n",
        "    if dev_acc > best_dev_acc:\n",
        "        best_dev_acc = dev_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f} | Train Acc={train_acc:.4f} | Dev Acc={dev_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfszA9UdGS1K",
        "outputId": "27449878-b916-4ff9-99f1-4420a134ceaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Dev Accuracy: 0.8724130663856692\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "final_dev_acc = evaluate(model, dev_loader)\n",
        "print(\"Final Dev Accuracy:\", final_dev_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu7Hcw9TGURC",
        "outputId": "04ef5fd0-48c5-4726-97ee-8d98645c9304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('i', 'PRON'), ('love', 'VERB'), ('NLP', 'VERB')]\n"
          ]
        }
      ],
      "source": [
        "def predict_sentence(sentence):\n",
        "    model.eval()\n",
        "    words = sentence.split()\n",
        "    indices = [word_to_ix.get(w, word_to_ix[\"<UNK>\"]) for w in words]\n",
        "    x = torch.tensor(indices).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        preds = torch.argmax(logits, dim=-1).squeeze(0)\n",
        "\n",
        "    ix2tag = {v: k for k, v in tag_to_ix.items()}\n",
        "    result = [(w, ix2tag[p.item()]) for w, p in zip(words, preds)]\n",
        "    return result\n",
        "print(predict_sentence(\"i love NLP\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En8Xrm81GYct"
      },
      "source": [
        "BÁO CÁO KẾT QUẢ:\n",
        "\n",
        "• Vocabulary size: 20201\n",
        "• Tag set size: 18\n",
        "\n",
        "Mô hình RNN được huấn luyện 3 epoch.\n",
        "Sau mỗi epoch,hàm loss, độ chính xác trên tập train và dev như sau:\n",
        "\n",
        "Epoch 1: Loss=0.9911 | Train Acc=0.7048 | Dev Acc=0.8074\n",
        "Epoch 2: Loss=0.5086 | Train Acc=0.8412 | Dev Acc=0.8508\n",
        "Epoch 3: Loss=0.3749 | Train Acc=0.8818 | Dev Acc=0.8724\n",
        "\n",
        "Dựa trên độ chính xác trên tập dev, mô hình có điểm dev cao nhất được lưu lại.\n",
        "Sau khi load mô hình tốt nhất:\n",
        "\n",
        "Final Dev Accuracy: 0.8724130663856692\n",
        "\n",
        "Hàm predict_sentence(sentence) được viết để dự đoán nhãn UPOS cho câu mới.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZqxS27HBN8"
      },
      "source": [
        "KẾT QUẢ THỰC HIỆN :\n",
        "- Độ chính xác trên tập dev : 0.8724...\n",
        "- Ví dụ dự đoán câu mới :\n",
        "  + Câu \"i love NLP\"\n",
        "  + Dự đoán : [('i', 'PRON'), ('love', 'VERB'), ('NLP', 'VERB')]\n",
        "  + Câu \"This is a test sentence\"\n",
        "  + Dự đoán : [('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('test', 'NOUN'), ('sentence', 'VERB')]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
